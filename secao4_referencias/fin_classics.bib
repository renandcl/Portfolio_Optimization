@article{markowitz1952portfolio,
   abstract = {The purpose of this paper is to present the likelihood methods for the analysis of cointegration in VAR models with Gaussian errors, seasonal dummies, and constant terms. We discuss likelihood ratio tests of cointegration rank and find the asymptotic distribution of the test statistics. We characterize the maximum likelihood estimator of the cointegrating relations and formulate tests of structural hypotheses about these relations. We show that the asymptotic distribution of the maximum likelihood estimator is mixed Gaussian. Once a certain eigenvalue problem is solved and the eigenvectors and eigenvalues calculated, one can conduct inference on the cointegrating rank using some nonstandard distributions, and test hypotheses about cointegrating relations using the x2 distribution.},
   author = {Harry Markowitz},
   doi = {10.2307/2975974},
   issn = {00221082, 15406261},
   issue = {1},
   journal = {The Journal of Finance},
   pages = {77-91},
   title = {Portfolio Selection},
   volume = {7},
   year = {1952},
}

@article{sharpe1964capital,
   author = {William F. Sharpe},
   doi = {10.1111/j.1540-6261.1964.tb02865.x},
   issn = {15406261},
   issue = {3},
   journal = {The Journal of Finance},
   pages = {425-442},
   title = {CAPITAL ASSET PRICES: A THEORY OF MARKET EQUILIBRIUM UNDER CONDITIONS OF RISK},
   volume = {19},
   year = {1964},
}

@article{lintner1965valuation,
   abstract = {N/A},
   author = {John Lintner},
   doi = {10.2307/1924119},
   issn = {00346535},
   issue = {1},
   journal = {The Review of Economics and Statistics},
   note = {Overview: Lintner discuss over the utility curve and the evaluation over market line, with a perspective of borrowing and lending.},
   title = {The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets},
   volume = {47},
   year = {1965},
}

@article{zadeh1965fuzzy,
   abstract = {A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint. © 1965 Academic Press, Inc.},
   author = {L. A. Zadeh},
   doi = {10.1016/S0019-9958(65)90241-X},
   issn = {00199958},
   issue = {3},
   journal = {Information and Control},
   title = {Fuzzy sets},
   volume = {8},
   year = {1965},
}


@article{fishburn1968utility,
   abstract = {Utility theory is interested in people's preferences or values and with assumptions about a person's preferences that enable them to be represented in numerically useful ways. The first two sections of this paper say more about what utility is, why people are interested in it, and how it is interpreted and used in the management and behavioral sciences. The third section summarizes a number of utility theories: it may be used either as a concluding overview of the range and variety of utility theories or as a bridge to the final sections. The final eight sections comprise a semi-technical survey of particular theories for readers interested in greater depth.},
   author = {Peter C Fishburn},
   issn = {00251909, 15265501},
   issue = {5},
   journal = {Management Science},
   pages = {335-378},
   publisher = {INFORMS},
   title = {Utility Theory},
   volume = {14},
   url = {http://www.jstor.org/stable/2628674},
   year = {1968},
}

@article{fama1970efficient,
   abstract = {N/A},
   author = {Eugene F. Fama},
   doi = {10.2307/2325486},
   issn = {00221082},
   issue = {2},
   journal = {The Journal of Finance},
   title = {Efficient Capital Markets: A Review of Theory and Empirical Work},
   volume = {25},
   year = {1970},
}

@article{merton1971optimal,
   author = {Robert C. Merton},
   doi = {10.1016/0022-0531(71)90038-X},
   issn = {10957235},
   issue = {4},
   journal = {Journal of Economic Theory},
   title = {Optimum consumption and portfolio rules in a continuous-time model},
   volume = {3},
   year = {1971},
}

@article{black1973pricing,
   abstract = {If options are correctly priced in the market, it should not be possible to make sure profits by creating portfolios of long and short positions in options and their underlying stocks. Using this principle, a theoretical valuation formula for options is derived. Since almost all corporate liabilities can be viewed as combinations of options, the formula and the analysis that led to it are also applicable to corporate liabilities such as common stock, corporate bonds, and warrants. In particular, the formula can be used to derive the discount that should be applied to a corporate bond because of the possibility of default.},
   author = {Fischer Black and Myron Scholes},
   doi = {10.1086/260062},
   issn = {1537534X},
   issue = {3},
   journal = {Journal of Political Economy},
   title = {The pricing of options and corporate liabilities},
   volume = {81},
   year = {1973},
}
@article{ross1976abitrage,
   author = {Stephen A. Ross},
   doi = {10.1016/0022-0531(76)90046-6},
   issn = {10957235},
   issue = {3},
   journal = {Journal of Economic Theory},
   title = {The arbitrage theory of capital asset pricing},
   volume = {13},
   year = {1976},
}

@article{engle1982autoregressive,
   abstract = {Traditional econometric models assume a constant one-period forecast variance. To generalize this implausible assumption, a new class of stochastic processes called autore-gressive conditional heteroscedastic (ARCH) processes are introduced in this paper. These are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance. A regression model is then introduced with disturbances following an ARCH process. Maximum likelihood estimators are described and a simple scoring iteration formulated. Ordinary least squares maintains its optimality properties in this set-up, but maximum likelihood is more efficient. The relative efficiency is calculated and can be infinite. To test whether the disturbances follow an ARCH process, the Lagrange multiplier procedure is employed. The test is based simply on the autocorrelation of the squared OLS residuals. This model is used to estimate the means and variances of inflation in the U.K. The ARCH effect is found to be significant and the estimated variances increase substantially during the chaotic seventies.},
   author = {Robert F. Engle},
   doi = {10.2307/1912773},
   issn = {00129682},
   issue = {4},
   journal = {Econometrica},
   title = {Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation},
   volume = {50},
   year = {1982},
}

@article{bollerslev1986generalized,
   abstract = {A natural generalization of the ARCH (Autoregressive Conditional Heteroskedastic) process introduced in Engle (1982) to allow for past conditional variances in the current conditional variance equation is proposed. Stationarity conditions and autocorrelation structure for this new class of parametric models are derived. Maximum likelihood estimation and testing are also considered. Finally an empirical example relating to the uncertainty of the inflation rate is presented. © 1986.},
   author = {Tim Bollerslev},
   doi = {10.1016/0304-4076(86)90063-1},
   issn = {03044076},
   issue = {3},
   journal = {Journal of Econometrics},
   title = {Generalized autoregressive conditional heteroskedasticity},
   volume = {31},
   year = {1986},
}

@article{davis1990portfolio,
   abstract = {In this paper, optimal consumption and investment decisions are studied for an investor who has available a bank account paying a fixed rate of interest and a stock whose price is a log-normal diff...},
   author = {M. H. A. Davis and A. R. Norman},
   doi = {10.1287/MOOR.15.4.676},
   isbn = {0780304500},
   issn = {01912216},
   journal = {https://doi.org/10.1287/moor.15.4.676},
   keywords = {Portfolio selection,free boundary problem,local time,reflecting diffusions,stochastic control,transaction costs},
   month = {11},
   pages = {1317-1320},
   publisher = { INFORMS },
   title = {Portfolio Selection with Transaction Costs},
   volume = {2},
   url = {https://pubsonline.informs.org/doi/abs/10.1287/moor.15.4.676},
   year = {1990},
}

@article{konno1991mean,
   abstract = {The purpose of this paper is to demonstrate that a ponfolio optimization model using the L, risk (mean absolute deviation risk) function can remove most of the difficulties as.sociated with the classical Markowit/'s model while maintaining its advantages over equilibrium models. In particular, the L, risk model leads to a linear program instead ofa quadralic program, so that a large-scale optimization problem consisting of more than 1,000 stocks may be solved on a real time basis. Numerical experiments using the historical data of NIKKEI 225 stocks show that the /., risk model generates a ponfolio quite similar to that ofthe Markowitz's model within a fraction of time required to solve the latter.},
   author = {Hiroshi Konno and Hiroaki Yamazaki},
   doi = {10.1287/mnsc.37.5.519},
   issn = {0025-1909},
   issue = {5},
   journal = {Management Science},
   pages = {519-531},
   title = {Mean-Absolute Deviation Portfolio Optimization Model and Its Applications to Tokyo Stock Market},
   volume = {37},
   year = {1991},
}

@book{jpmorgan1996riskmetrics,
   abstract = {This Technical Document provides a detailed description of RiskMetrics ™ , a set of techniques and data to measure market risks in portfolios of fixed income instruments, equities, foreign exchange, commod- ities, and their derivatives issued in over 30 countries. This edition has been expanded significantly from the previous release issued in May 1995.},
   author = {J. P. Morgan and Reuters Ltd},
   city = {New York},
   edition = {4},
   editor = {Jacques Longerstaey and Martin Spencer},
   journal = {Morgan Guaranty Trust},
   publisher = {Morgan Guaranty Trust Company},
   title = {RiskMetrics - Technical Document},
   year = {1996},
}

@article{artzner1999coherent,
   abstract = {In this paper we study both market risks and nonmarket risks, without complete markets assumption, and discuss methods of measurement of these risks. We present and justify a set of four desirable properties for measures of risk, and call the measures satisfying these properties "coherent." We examine the measures of risk provided and the related actions required by SPAN, by the SEC/NASD rules, and by quantile-based methods. We demonstrate the universality of scenario-based methods for providing coherent measures. We offer suggestions concerning the SEC method. We also suggest a method to repair the failure of subadditivity of quantile-based methods.},
   author = {Philippe Artzner and Freddy Delbaen and Jean Marc Eber and David Heath},
   doi = {10.1111/1467-9965.00068},
   issn = {09601627},
   issue = {3},
   journal = {Mathematical Finance},
   note = {Overview: Artzner discuss over risk understanding, the acceptance and perspectives. as VaR},
   title = {Coherent measures of risk},
   volume = {9},
   year = {1999},
}

@article{rockafellar2000optimization,
   abstract = {A new approach to optimizing or hedging a portfolio of financial instruments to reduce risk is presented and tested on applications. It focuses on minimizing conditional value-at-risk (CVaR) rather than minimizing value-at-risk (VaR), but portfolios with low CVaR necessarily have low VaR as well. CVaR, also called mean excess loss, mean shortfall, or tail VaR, is in any case considered to be a more consistent measure of risk than VaR. Central to the new approach is a technique for portfolio optimization which calculates VaR and optimizes CVaR simultaneously. This technique is suitable for use by investment companies, brokerage firms, mutual funds, and any business that evaluates risk. It can be combined with analytical or scenario-based methods to optimize portfolios with large numbers of instruments, in which case the calculations often come down to linear programming or nonsmooth programming. The methodology can also be applied to the optimization of percentiles in contexts outside of finance.},
   author = {R. Tyrrell Rockafellar and Stanislav Uryasev},
   doi = {10.21314/jor.2000.038},
   issn = {14651211},
   issue = {3},
   journal = {The Journal of Risk},
   note = {Author propose the application of a new risk measure to optmize the portfolio, by minimizing the Conditional Value-at-Risk, allowing the application of linear programming on calculations.<br/>In sum, the Conditional Value-at-Risk, is the mean value of all values lower then Value-at-Risk, which is the minimum value on the confidence level beta.<br/>This article demonstrate mathematically the functions and the it is convex and continuously differentiable, and that minimizing the loss CVaR is the same as minimizing the all the density probability (x, alpha).},
   pages = {21-41},
   title = {Optimization of conditional value-at-risk},
   volume = {2},
   year = {2000},
}

@inproceedings{uryasev2000conditional,
   abstract = {A new approach for the simultaneous calculation of Value-at-Risk (VaR) and optimization of Conditional Value-at-Risk (CVaR) for a broad class of problems is presented. It is shown that CVaR can be efficiently minimized using linear programming (LP) techniques. Although, formally, the method minimizes only CVaR, it also lowers VaR.},
   author = {Stanislav Uryasev},
   doi = {10.1109/cifer.2000.844598},
   journal = {IEEE/IAFE Conference on Computational Intelligence for Financial Engineering, Proceedings (CIFEr)},
   title = {Conditional Value-at-Risk: optimization algorithms and applications},
   year = {2000},
}

@inproceedings{fama2004capital,
   author = {Eugene F. Fama and Kenneth R. French},
   doi = {10.1257/0895330042162430},
   issn = {08953309},
   issue = {3},
   journal = {Journal of Economic Perspectives},
   title = {The Capital Asset Pricing Model: Theory and evidence},
   volume = {18},
   year = {2004},
}

@article{demiguel2009generalized,
   abstract = {We provide a general framework for finding portfolios that perform well out-of-sample in the presence of estimation error. This framework relies on solving the traditional minimum-variance problem but subject to the additional constraint that the norm of the portfolio-weight vector be smaller than a given threshold. We show that our framework nests as special cases the shrinkage approaches of Jagannathan and Ma (Jagannathan, R., T. Ma. 2003. Risk reduction in large portfolios: Why imposing the wrong constraints helps. J. Finance 58 1651-1684) and Ledoit and Wolf (Ledoit, O., M. Wolf. 2003. Improved estimation of the covariance matrix of stock returns with an application to portfolio selection. J. Empirical Finance 10 603-621, and Ledoit, O., M. Wolf. 2004. A well-conditioned estimator for large-dimensional covariance matrices. J. Multivariate Anal. 88 365-411) and the 1/N portfolio studied in DeMiguel et al. (DeMiguel, V., L. Garlappi, R. Uppal. 2009. Optimal versus naive diversification: How inefficient is the 1/N portfolio strategy? Rev. Financial Stud. 22 1915-1953). We also use our framework to propose several new portfolio strategies. For the proposed portfolios, we provide a momentshrinkage interpretation and a Bayesian interpretation where the investor has a prior belief on portfolio weights rather than on moments of asset returns. Finally, we compare empirically the out-of-sample performance of the new portfolios we propose to 10 strategies in the literature across five data sets. We find that the norm-constrained portfolios often have a higher Sharpe ratio than the portfolio strategies in Jagannathan and Ma (2003), Ledoit and Wolf (2003, 2004), the 1/N portfolio, and other strategies in the literature, such as factor portfolios. © 2009 INFORMS.},
   author = {Victor DeMiguel and Lorenzo Garlappi and Francisco J. Nogales and Raman Uppal},
   doi = {10.1287/mnsc.1080.0986},
   issn = {00251909},
   issue = {5},
   journal = {Management Science},
   title = {A generalized approach to portfolio optimization: Improving performance by constraining portfolio norms},
   volume = {55},
   year = {2009},
}

@article{demiguel2009optimal,
   abstract = {We evaluate the out-of-sample performance of the sample-based mean-variance model, and its extensions designed to reduce estimation error, relative to the naive 1/N portfolio. Of the 14 models we evaluate across seven empirical datasets, none is consistently better than the 1/N rule in terms of Sharpe ratio, certainty-equivalent return, or turnover, which indicates that, out of sample, the gain from optimal diversification is more than offset by estimation error. Based on parameters calibrated to the US equity market, our analytical results and simulations show that the estimation window needed for the sample-based mean-variance strategy and its extensions to outperform the 1/N benchmark is around 3000 months for a portfolio with 25 assets and about 6000 months for a portfolio with 50 assets. This suggests that there are still many "miles to go" before the gains promised by optimal portfolio choice can actually be realized out of sample.},
   author = {Victor DeMiguel and Lorenzo Garlappi and Raman Uppal},
   doi = {10.1093/rfs/hhm075},
   issn = {14657368},
   issue = {5},
   journal = {Review of Financial Studies},
   title = {Optimal versus naive diversification: How inefficient is the 1/N portfolio strategy?},
   volume = {22},
   year = {2009},
}


@article{garleanu2013dynamic,
   abstract = {We derive a closed-form optimal dynamic portfolio policy when trading is costly and security returns are predictable by signals with different mean-reversion speeds. The optimal strategy is characterized by two principles: (1) aim in front of the target, and (2) trade partially toward the current aim. Specifically, the optimal updated portfolio is a linear combination of the existing portfolio and an "aim portfolio," which is a weighted average of the current Markowitz portfolio (the moving target) and the expected Markowitz portfolios on all future dates (where the target is moving). Intuitively, predictors with slower mean-reversion (alpha decay) get more weight in the aim portfolio. We implement the optimal strategy for commodity futures and find superior net returns relative to more naive benchmarks. © 2013 The American Finance Association.},
   author = {Nicolae Gârleanu and Lasse Heje Pedersen},
   doi = {10.1111/jofi.12080},
   issn = {15406261},
   issue = {6},
   journal = {Journal of Finance},
   title = {Dynamic Trading with Predictable Returns and Transaction Costs},
   volume = {68},
   year = {2013},
}


@article{sharpe1994sharpe,
   abstract = {Over twenty-five years ago, in sharpe (1966) I introduced a measure for the performance of mutual funds and proposed the term reward-to-variability ratio to descripe it (the measure is also descriped in Sharpe [1975]). While the measure has gained considerable popularity, the name has not. Other authors have termed the original version the Sharpe index (Radcliff [1990, p. 286] and Haugen [1993, p. 315]), the Sharpe Measure (Bodie, Kane, and Marcus [1993, p. 804], Elton and Gruber [1991, p. 652], and Reilly [1993, p. 24]), or the Sharpe Ratio (Morningstar [1993, p. 24]). Generalized versions have also appeared under various names (see, for example, BARRA [1992, P. 21] and Capaul, Rowley, and Sharpe [1993, p. 33]). Bowing to increasingly common usage, this article refers to both the original measure and more generalized versions as the Sharpe Ratio. My goal here is to go well beyond the discussion of the original measure in Sharpe [1966] and Sharpe [1975], providing more generality and covering a broader range of applications.},
   author = {William F. Sharpe},
   doi = {10.3905/jpm.1994.409501},
   issn = {0095-4918},
   issue = {1},
   journal = {The Journal of Portfolio Management},
   title = {The Sharpe Ratio},
   volume = {21},
   year = {1994},
}


@article{treynor1973use,
   abstract = {The capital asset pricing model suggests that any premium for risk bearing will be associated with market, rather than specific risk. If investors in the aggregate are risk averse, then an investment in the market asset-explicit or implicit-offers a premium. A portfolio devoid of specific risk is called "perfectly diversified." In other words "perfect diversification" does not mean the absence of risk nor does it mean an optimally balanced portfolio, except in the case of zero. In general, a given security may play two different roles simultaneously. It may hold a temporary position based entirely on expected independent return and appraisal risk. As price fluctuates and the investor's information changes, the optimum position changes. It may also hold a position resulting purely from the fact that the security in question constitutes part of the market portfolio. The latter position changes as market expectations change but is virtually independent of expectations regarding independent return on the security.},
   author = {Jack L. Treynor and Fischer Black},
   doi = {10.1086/295508},
   issn = {0021-9398},
   issue = {1},
   journal = {The Journal of Business},
   title = {How to Use Security Analysis to Improve Portfolio Selection},
   volume = {46},
   year = {1973},
}


